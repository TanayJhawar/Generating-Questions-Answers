{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b743564",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import *\n",
    "from tkinter import messagebox\n",
    "from PIL import Image,ImageTk\n",
    "import tkinter as tk\n",
    "from tkinter import simpledialog\n",
    "import pandas as pd\n",
    "from IPython.display import Markdown, display, clear_output\n",
    "import _pickle as cPickle\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "import os\n",
    "import gensim\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re\n",
    "stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e585a7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dumpPickle(fileName, content):\n",
    "    pickleFile = open(fileName, 'wb')\n",
    "    cPickle.dump(content, pickleFile, -1)\n",
    "    pickleFile.close()\n",
    "\n",
    "def loadPickle(fileName):    \n",
    "    file = open(fileName, 'rb')\n",
    "    content = cPickle.load(file)\n",
    "    file.close()\n",
    "    \n",
    "    return content\n",
    "    \n",
    "def pickleExists(fileName):\n",
    "    file = Path(fileName)\n",
    "    \n",
    "    if file.is_file():\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "#Save named entities start points\n",
    "\n",
    "def getNEStartIndexs(doc):\n",
    "    neStarts = {}\n",
    "    for ne in doc.ents:\n",
    "        neStarts[ne.start] = ne\n",
    "        \n",
    "    return neStarts \n",
    "\n",
    "def getSentenceStartIndexes(doc):\n",
    "    senStarts = []\n",
    "    \n",
    "    for sentence in doc.sents:\n",
    "        senStarts.append(sentence[0].i)\n",
    "    \n",
    "    return senStarts\n",
    "    \n",
    "def getSentenceForWordPosition(wordPos, senStarts):\n",
    "    for i in range(1, len(senStarts)):\n",
    "        if (wordPos < senStarts[i]):\n",
    "            return i - 1\n",
    "        \n",
    "def addWordsForParagrapgh(newWords, text):\n",
    "    doc = nlp(text)\n",
    "\n",
    "    neStarts = getNEStartIndexs(doc)\n",
    "    senStarts = getSentenceStartIndexes(doc)\n",
    "    \n",
    "    #index of word in spacy doc text\n",
    "    i = 0\n",
    "    \n",
    "    while (i < len(doc)):\n",
    "        #If the token is a start of a Named Entity, add it and push to index to end of the NE\n",
    "        if (i in neStarts):\n",
    "            word = neStarts[i]\n",
    "            #add word\n",
    "            currentSentence = getSentenceForWordPosition(word.start, senStarts)\n",
    "            wordLen = word.end - word.start\n",
    "            shape = ''\n",
    "            for wordIndex in range(word.start, word.end):\n",
    "                shape += (' ' + doc[wordIndex].shape_)\n",
    "\n",
    "            newWords.append([word.text,\n",
    "                            0,\n",
    "                            0,\n",
    "                            currentSentence,\n",
    "                            wordLen,\n",
    "                            word.label_,\n",
    "                            None,\n",
    "                            None,\n",
    "                            None,\n",
    "                            shape])\n",
    "            i = neStarts[i].end - 1\n",
    "        #If not a NE, add the word if it's not a stopword or not regular letters\n",
    "        else:\n",
    "            if (doc[i].is_stop == False and doc[i].is_alpha == True):\n",
    "                word = doc[i]\n",
    "\n",
    "                currentSentence = getSentenceForWordPosition(i, senStarts)\n",
    "                wordLen = 1\n",
    "\n",
    "                newWords.append([word.text,\n",
    "                                0,\n",
    "                                0,\n",
    "                                currentSentence,\n",
    "                                wordLen,\n",
    "                                None,\n",
    "                                word.pos_,\n",
    "                                word.tag_,\n",
    "                                word.dep_,\n",
    "                                word.shape_])\n",
    "        i += 1\n",
    "\n",
    "def oneHotEncodeColumns(df):\n",
    "    columnsToEncode = ['NER', 'POS', \"TAG\", 'DEP']\n",
    "\n",
    "    for column in columnsToEncode:\n",
    "        one_hot = pd.get_dummies(df[column])\n",
    "        one_hot = one_hot.add_prefix(column + '_')\n",
    "\n",
    "        df = df.drop(column, axis = 1)\n",
    "        df = df.join(one_hot)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def generateDf(text):\n",
    "    words = []\n",
    "    addWordsForParagrapgh(words, text)\n",
    "\n",
    "    wordColums = ['text', 'titleId', 'paragrapghId', 'sentenceId','wordCount', 'NER', 'POS', 'TAG', 'DEP','shape']\n",
    "    df = pd.DataFrame(words, columns=wordColums)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def prepareDf(df):\n",
    "    #One-hot encoding\n",
    "    wordsDf = oneHotEncodeColumns(df)\n",
    "\n",
    "\n",
    "    #Add missing colums \n",
    "    predictorFeaturesName = 'data/pickles/nb-predictor-features.pkl'\n",
    "    featureNames = loadPickle(predictorFeaturesName)\n",
    "\n",
    "    for feature in featureNames:\n",
    "        if feature not in wordsDf.columns:\n",
    "            wordsDf[feature] = 0    \n",
    "                \n",
    "    #Drop unused columns\n",
    "    columnsToDrop = ['text', 'titleId', 'paragrapghId', 'sentenceId', 'shape', 'isAnswer']\n",
    "    wordsDf = wordsDf.drop(columnsToDrop, axis = 1)\n",
    "\n",
    "\n",
    "    return wordsDf\n",
    "\n",
    "def predictWords(wordsDf, df):\n",
    "    \n",
    "    predictorPickleName = 'data/pickles/nb-predictor.pkl'\n",
    "    predictor = loadPickle(predictorPickleName)\n",
    "    \n",
    "    y_pred = predictor.predict_proba(wordsDf)\n",
    "\n",
    "    labeledAnswers = []\n",
    "    for i in range(len(y_pred)):\n",
    "        labeledAnswers.append({'word': df.iloc[i]['text'], 'prob': y_pred[i][0]})\n",
    "    \n",
    "    return labeledAnswers\n",
    "\n",
    "def blankAnswer(firstTokenIndex, lastTokenIndex, sentStart, sentEnd, doc):\n",
    "    leftPartStart = doc[sentStart].idx\n",
    "    leftPartEnd = doc[firstTokenIndex].idx\n",
    "    rightPartStart = doc[lastTokenIndex].idx + len(doc[lastTokenIndex])\n",
    "    rightPartEnd = doc[sentEnd - 1].idx + len(doc[sentEnd - 1])\n",
    "    \n",
    "    question = doc.text[leftPartStart:leftPartEnd] + '_____' + doc.text[rightPartStart:rightPartEnd]\n",
    "    \n",
    "    return question\n",
    "\n",
    "def addQuestions(answers, text):\n",
    "    doc = nlp(text)\n",
    "    currAnswerIndex = 0\n",
    "    qaPair = []\n",
    "\n",
    "    #Check wheter each token is the next answer\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "            \n",
    "            #If all the answers have been found, stop looking\n",
    "            if currAnswerIndex >= len(answers):\n",
    "                break\n",
    "            \n",
    "            #In the case where the answer is consisted of more than one token, check the following tokens as well.\n",
    "            answerDoc = nlp(answers[currAnswerIndex]['word'])\n",
    "            answerIsFound = True\n",
    "            \n",
    "            for j in range(len(answerDoc)):\n",
    "                if token.i + j >= len(doc) or doc[token.i + j].text != answerDoc[j].text:\n",
    "                    answerIsFound = False\n",
    "           \n",
    "            #If the current token is corresponding with the answer, add it \n",
    "            if answerIsFound:\n",
    "                question = blankAnswer(token.i, token.i + len(answerDoc) - 1, sent.start, sent.end, doc)\n",
    "                \n",
    "                qaPair.append({'question' : question, 'answer': answers[currAnswerIndex]['word'], 'prob': answers[currAnswerIndex]['prob']})\n",
    "                \n",
    "                currAnswerIndex += 1\n",
    "                \n",
    "    return qaPair\n",
    "\n",
    "def sortAnswers(qaPairs):\n",
    "    orderedQaPairs = sorted(qaPairs, key=lambda qaPair: qaPair['prob'])\n",
    "    \n",
    "    return orderedQaPairs   \n",
    "\n",
    "glove_file = 'data/embeddings/glove.6B.300d.txt'\n",
    "tmp_file = 'data/embeddings/word2vec-glove.6B.300d.txt'\n",
    "model = None\n",
    "\n",
    "if os.path.isfile(glove_file):\n",
    "    from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "    model = KeyedVectors.load_word2vec_format(tmp_file)\n",
    "else:\n",
    "    print(\"Glove embeddings not found. Please download and place them in the following path: \" + glove_file)\n",
    "\n",
    "def generate_distractors(answer, count):\n",
    "    answer = str.lower(answer)\n",
    "    \n",
    "    ##Extracting closest words for the answer. \n",
    "    try:\n",
    "        closestWords = model.most_similar(positive=[answer], topn=count)\n",
    "    except:\n",
    "        #In case the word is not in the vocabulary, or other problem not loading embeddings\n",
    "        return []\n",
    "\n",
    "    #Return count many distractors\n",
    "    distractors = list(map(lambda x: x[0], closestWords))[0:count]\n",
    "    \n",
    "    return distractors\n",
    "\n",
    "def addDistractors(qaPairs, count):\n",
    "    if not model:\n",
    "        print(\"Glove embeddings not found. Please download and place them in the following path: \" + glove_file)\n",
    "    \n",
    "    for qaPair in qaPairs:\n",
    "        distractors = generate_distractors(qaPair['answer'], count)\n",
    "        qaPair['distractors'] = distractors\n",
    "    \n",
    "    return qaPairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bb652d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateQuestions(text, count):\n",
    "    \n",
    "    # Extract words \n",
    "    df = generateDf(text)\n",
    "    wordsDf = prepareDf(df)\n",
    "    \n",
    "    # Predict \n",
    "    labeledAnswers = predictWords(wordsDf, df)\n",
    "    \n",
    "    # Transform questions\n",
    "    qaPairs = addQuestions(labeledAnswers, text)\n",
    "    \n",
    "    # Pick the best questions\n",
    "    orderedQaPairs = sortAnswers(qaPairs)\n",
    "    \n",
    "    # Generate distractors\n",
    "    questions = addDistractors(orderedQaPairs[:count], 4)\n",
    "    \n",
    "    return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41109185",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = LancasterStemmer()\n",
    "\n",
    "# List to hold all input sentences\n",
    "sentences = []\n",
    "\n",
    "# Dictionary to hold sentences corresponding to respective discourse markers\n",
    "disc_sentences = {}\n",
    "\n",
    "# Remaining sentences which do not have discourse markers (To be used later to generate other kinds of questions)\n",
    "nondisc_sentences = []\n",
    "\n",
    "# List of auxiliary verbs\n",
    "aux_list = ['am', 'are', 'is', 'was', 'were', 'can', 'could', 'does', 'do', 'did', 'has', 'had', 'may', 'might', 'must',\n",
    "            'need', 'ought', 'shall', 'should', 'will', 'would']\n",
    "\n",
    "# List of all discourse markers\n",
    "discourse_markers = ['because', 'as a result', 'since', 'when', 'although', 'for example', 'for instance']\n",
    "\n",
    "# Different question types possible for each discourse marker\n",
    "qtype = {'because': ['Why'], 'since': ['When', 'Why'], 'when': ['When'], 'although': ['Yes/No'], 'as a result': ['Why'], \n",
    "        'for example': ['Give an example where'], 'for instance': ['Give an instance where'], 'to': ['Why']}\n",
    "\n",
    "# The argument which forms a question\n",
    "target_arg = {'because': 1, 'since': 1, 'when': 1, 'although': 1, 'as a result': 2, 'for example': 1, 'for instance': 1, \n",
    "              'to': 1}\n",
    "\n",
    "def sentensify():\n",
    "    global sentences\n",
    "    fp = open('input.txt')\n",
    "    data = fp.read()\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentences = tokenizer.tokenize(data)\n",
    "    questions = discourse()\n",
    "    return questions\n",
    "    \n",
    "def generate_question(question_part, type):\n",
    "\n",
    "    ''' \n",
    "        question_part -> Part of input sentence which forms a question\n",
    "        type-> The type of question (why, where, etc)\n",
    "    '''\n",
    "    # Remove full stop and make first letter lower case\n",
    "    question_part = question_part[0].lower() + question_part[1:]\n",
    "    if(question_part[-1] == '.' or question_part[-1] == ','):\n",
    "        question_part = question_part[:-1]\n",
    "        \n",
    "    # Capitalizing 'i' since 'I' is recognized by parsers appropriately    \n",
    "    for i in range(0, len(question_part)):\n",
    "        if(question_part[i] == 'i'):\n",
    "            if((i == 0 and question_part[i+1] == ' ') or (question_part[i-1] == ' ' and question_part[i+1] == ' ')):\n",
    "                question_part = question_part[:i] + 'I' + question_part[i + 1: ]\n",
    "                \n",
    "    question = \"\"\n",
    "    if(type == 'Give an example where' or type == 'Give an instance where'):\n",
    "        question = type + \" \" + question_part + '?'\n",
    "        return question\n",
    "\n",
    "    aux_verb = False\n",
    "    res = None\n",
    "    \n",
    "    # Find out if auxiliary verb already exists\n",
    "    for i in range(len(aux_list)):\n",
    "        if(aux_list[i] in question_part.split()):\n",
    "            aux_verb = True\n",
    "            pos = i\n",
    "            break\n",
    "\n",
    "    # If auxiliary verb exists\n",
    "    if(aux_verb):\n",
    "        \n",
    "        # Tokeninze the part of the sentence from which the question has to be made\n",
    "        text = nltk.word_tokenize(question_part)\n",
    "        tags = nltk.pos_tag(text)\n",
    "        question_part = \"\"\n",
    "        fP = False\n",
    "        \n",
    "        for word, tag in tags:\n",
    "            if(word in ['I', 'We', 'we']):\n",
    "                question_part += 'you' + \" \"\n",
    "                fP = True\n",
    "                continue\n",
    "            question_part += word + \" \"\n",
    "\n",
    "        # Split across the auxiliary verb and prepend it at the start of question part\n",
    "        question = question_part.split(\" \" + aux_list[pos])\n",
    "        if(fP):\n",
    "             question = [\"were \"] + question\n",
    "        else:\n",
    "            question = [aux_list[pos] + \" \"] + question\n",
    "\n",
    "        # If Yes/No, no need to introduce question phrase\n",
    "        if(type == 'Yes/No'):\n",
    "            question += ['?']\n",
    "            \n",
    "        elif(type != \"non_disc\"):\n",
    "            question = [type + \" \"] + question + [\"?\"]\n",
    "            \n",
    "        else:\n",
    "            question = question + [\"?\"]\n",
    "         \n",
    "        question = ''.join(question)\n",
    "\n",
    "    # If auxilary verb does ot exist, it can only be some form of verb 'do'\n",
    "    else:\n",
    "        aux = None\n",
    "        text = nltk.word_tokenize(question_part)\n",
    "        tags = nltk.pos_tag(text)\n",
    "        comb = \"\"\n",
    "\n",
    "        '''There can be following combinations of nouns and verbs:\n",
    "            NN/NNP and VBZ  -> Does\n",
    "            NNS/NNPS(plural) and VBP -> Do\n",
    "            NN/NNP and VBN -> Did\n",
    "            NNS/NNPS(plural) and VBN -> Did\n",
    "        '''\n",
    "        \n",
    "        for tag in tags:\n",
    "            if(comb == \"\"):\n",
    "                if(tag[1] == 'NN' or tag[1] == 'NNP'):\n",
    "                    comb = 'NN'\n",
    "\n",
    "                elif(tag[1] == 'NNS' or tag[1] == 'NNPS'):\n",
    "                    comb = 'NNS'\n",
    "\n",
    "                elif(tag[1] == 'PRP'):\n",
    "                    if tag[0] in ['He','She','It']:\n",
    "                        comb = 'PRPS'\n",
    "                    else:\n",
    "                        comb = 'PRPP'\n",
    "                        tmp = question_part.split(\" \")\n",
    "                        tmp = tmp[1: ]\n",
    "                        if(tag[0] in ['I', 'we', 'We']):\n",
    "                            question_part = 'you ' + ' '.join(tmp)\n",
    "                            \n",
    "            if(res == None):\n",
    "                res = re.match(r\"VB*\", tag[1])\n",
    "                if(res):\n",
    "                    \n",
    "                    # Stem the verb\n",
    "                    question_part = question_part.replace(tag[0], stemmer.stem(tag[0]))\n",
    "                res = re.match(r\"VBN\", tag[1])\n",
    "                res = re.match(r\"VBD\", tag[1])\n",
    "\n",
    "        if(comb == 'NN'):\n",
    "            aux = 'does'\n",
    "            \n",
    "        elif(comb == 'NNS'):\n",
    "            aux = 'do'\n",
    "            \n",
    "        elif(comb == 'PRPS'):\n",
    "            aux = 'does'\n",
    "            \n",
    "        elif(comb == 'PRPP'):\n",
    "            aux = 'do'\n",
    "            \n",
    "        if(res and res.group() in ['VBD', 'VBN']):\n",
    "            aux = 'did'\n",
    "\n",
    "        if(aux):\n",
    "            if(type == \"non_disc\" or type == \"Yes/No\"):\n",
    "                question = aux + \" \" + question_part + \"?\"\n",
    "\n",
    "            else:\n",
    "                question = type + \" \" + aux + \" \" + question_part + \"?\"\n",
    "    if(question != \"\"):\n",
    "        question = question[0].upper() + question[1:]\n",
    "    return question\n",
    "\n",
    "def get_named_entities(sent):\n",
    "    doc = nlp(sent)\n",
    "    named_entities = [(X.text, X.label_) for X in doc.ents]\n",
    "    return named_entities\n",
    "\n",
    "def get_wh_word(entity, sent):\n",
    "    wh_word = \"\"\n",
    "    if entity[1] in ['TIME', 'DATE']:\n",
    "        wh_word = 'When'\n",
    "        \n",
    "    elif entity[1] == ['PRODUCT', 'EVENT', 'WORK_OF_ART', 'LAW', 'LANGUAGE']:\n",
    "        wh_word = 'What'\n",
    "        \n",
    "    elif entity[1] in ['PERSON']:\n",
    "            wh_word = 'Who'\n",
    "            \n",
    "    elif entity[1] in ['NORP', 'FAC' ,'ORG', 'GPE', 'LOC']:\n",
    "        index = sent.find(entity[0])\n",
    "        if index == 0:\n",
    "            wh_word = \"Who\"\n",
    "            \n",
    "        else:\n",
    "            wh_word = \"Where\"\n",
    "            \n",
    "    else:\n",
    "        wh_word = \"Where\"\n",
    "    return wh_word\n",
    "\n",
    "def generate_one_word_questions(sent):\n",
    "    \n",
    "    named_entities = get_named_entities(sent)\n",
    "    questions = []\n",
    "    \n",
    "    if not named_entities:\n",
    "        return questions\n",
    "    \n",
    "    for entity in named_entities:\n",
    "        wh_word = get_wh_word(entity, sent)\n",
    "        \n",
    "        if(sent[-1] == '.'):\n",
    "            sent = sent[:-1]\n",
    "        \n",
    "        if sent.find(entity[0]) == 0:\n",
    "            questions.append(sent.replace(entity[0],wh_word) + '?')\n",
    "            continue\n",
    "       \n",
    "        question = \"\"\n",
    "        aux_verb = False\n",
    "        res = None\n",
    "\n",
    "        for i in range(len(aux_list)):\n",
    "            if(aux_list[i] in sent.split()):\n",
    "                aux_verb = True\n",
    "                pos = i\n",
    "                break\n",
    "            \n",
    "        if not aux_verb:\n",
    "            pos = 9\n",
    "        \n",
    "        text = nltk.word_tokenize(sent)\n",
    "        tags = nltk.pos_tag(text)\n",
    "        question_part = \"\"\n",
    "        \n",
    "        if wh_word == 'When':\n",
    "            word_list = sent.split(entity[0])[0].split()\n",
    "            if word_list[-1] in ['in', 'at', 'on']:\n",
    "                question_part = \" \".join(word_list[:-1])\n",
    "            else:\n",
    "                question_part = \" \".join(word_list)\n",
    "            \n",
    "            qp_text = nltk.word_tokenize(question_part)\n",
    "            qp_tags = nltk.pos_tag(qp_text)\n",
    "            \n",
    "            question_part = \"\"\n",
    "            \n",
    "            for i, grp in enumerate(qp_tags):\n",
    "                word = grp[0]\n",
    "                tag = grp[1]\n",
    "                if(re.match(\"VB*\", tag) and word not in aux_list):\n",
    "                    question_part += WordNetLemmatizer().lemmatize(word,'v') + \" \"\n",
    "                else:\n",
    "                    question_part += word + \" \"\n",
    "                \n",
    "            if question_part[-1] == ' ':\n",
    "                question_part = question_part[:-1]\n",
    "        \n",
    "        else:\n",
    "            for i, grp in enumerate(tags):\n",
    "                \n",
    "                #Break the sentence after the first non-auxiliary verb\n",
    "                word = grp[0]\n",
    "                tag = grp[1]\n",
    "\n",
    "                if(re.match(\"VB*\", tag) and word not in aux_list):\n",
    "                    question_part += word\n",
    "\n",
    "                    if i<len(tags) and 'NN' not in tags[i+1][1] and wh_word != 'When':\n",
    "                        question_part += \" \"+ tags[i+1][0]\n",
    "\n",
    "                    break\n",
    "                question_part += word + \" \"\n",
    "        question = question_part.split(\" \"+ aux_list[pos])\n",
    "        question = [aux_list[pos] + \" \"] + question\n",
    "        question = [wh_word+ \" \"] + question + [\"?\"]\n",
    "        question = ''.join(question)\n",
    "        questions.append(question)\n",
    "    \n",
    "    return questions        \n",
    "\n",
    "def discourse():\n",
    "    temp = []\n",
    "    target = \"\"\n",
    "    questions = []\n",
    "    global disc_sentences\n",
    "    disc_sentences = {}\n",
    "    for i in range(len(sentences)):\n",
    "        maxLen = 9999999\n",
    "        val = -1\n",
    "        for j in discourse_markers:\n",
    "            tmp = len(sentences[i].split(j)[0].split(' '))  \n",
    "            \n",
    "            # To get valid, first discourse marker.   \n",
    "            if(len(sentences[i].split(j)) > 1 and tmp >= 3 and tmp < maxLen):\n",
    "                maxLen = tmp\n",
    "                val = j\n",
    "                \n",
    "        if(val != -1):\n",
    "\n",
    "            # To initialize a list for every new key\n",
    "            if(disc_sentences.get(val, 'empty') == 'empty'):\n",
    "                disc_sentences[val] = []\n",
    "                \n",
    "            disc_sentences[val].append(sentences[i])\n",
    "            temp.append(sentences[i])\n",
    "\n",
    "\n",
    "    nondisc_sentences = list(set(sentences) - set(temp))\n",
    "    \n",
    "    t = []\n",
    "    for k, v in disc_sentences.items():\n",
    "        for val in range(len(v)):\n",
    "            \n",
    "            # Split the sentence on discourse marker and identify the question part\n",
    "            question_part = disc_sentences[k][val].split(k)[target_arg[k] - 1]\n",
    "            q = generate_question(question_part, qtype[k][0])\n",
    "            if(q != \"\"):\n",
    "                questions.append([disc_sentences[k][val],q])\n",
    "                \n",
    "                \n",
    "    for question_part in nondisc_sentences:\n",
    "        s = \"non_disc\"\n",
    "        sentence = question_part\n",
    "        text = nltk.word_tokenize(question_part)\n",
    "        if(text[0] == 'Yes'):\n",
    "            question_part = question_part[5:]\n",
    "            s = \"Yes/No\"\n",
    "            \n",
    "        elif(text[0] == 'No'):\n",
    "            question_part = question_part[4:]\n",
    "            s = \"Yes/No\"\n",
    "            \n",
    "        q = generate_question(question_part, s)\n",
    "        if(q != \"\"):\n",
    "            questions.append([sentence,q])\n",
    "        l = generate_one_word_questions(question_part)\n",
    "        questions += [[sentence,i] for i in l]\n",
    "    #print(len(questions))\n",
    "    return(questions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
